{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import os\n",
    "from string import ascii_lowercase\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "home_dir = r'c:\\\\Users\\\\tutha\\\\Desktop\\\\Hacked2023\\\\ASL_Translation-NAIT\\\\'\n",
    "ds_dir = r'c:\\\\Users\\\\tutha\\\\Desktop\\\\Hacked2023\\\\ASL_Translation-NAIT\\\\dataset\\\\'\n",
    "newcoming_ds_dir = r'c:\\\\Users\\\\tutha\\\\Desktop\\\\Hacked2023\\\\ASL_Translation-NAIT\\\\letters\\\\'\n",
    "old_ds_dir = r'c:\\\\Users\\\\tutha\\\\Desktop\\\\Hacked2023\\\\ASL_Translation-NAIT\\\\old_dataset\\\\'\n",
    "\n",
    "for c in ascii_lowercase:\n",
    "    \n",
    "    combined_csv = pd.concat([pd.read_csv(old_ds_dir + c + \".csv\", header = None), pd.read_csv(newcoming_ds_dir + c + \".csv\", header = None)], axis=0, ignore_index=True)\n",
    "    combined_csv.to_csv(ds_dir + c +\".csv\", index=False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = {\"data\":[], \"label\":[]}\n",
    "        self.length = 0\n",
    "        class_index = 0\n",
    "        for c in ascii_lowercase:\n",
    "            with open(ds_dir + c + \".csv\") as csv_file:\n",
    "                reader = csv.reader(csv_file)\n",
    "                for row in reader:\n",
    "                    int_row = [int(element) for element in row]\n",
    "                    self.data[\"data\"].append(torch.tensor(int_row, dtype=torch.float32))\n",
    "                    self.data[\"label\"].append(torch.tensor(class_index))\n",
    "                    self.length += 1\n",
    "            class_index += 1\n",
    "\n",
    "        for i in range(len(self.data[\"data\"])):\n",
    "            self.data[\"data\"][i] /=  4095\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[\"data\"][index].unsqueeze(0), self.data[\"label\"][index]\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44071"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CustomDataset()\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[1.0000, 0.3746, 0.7140, 0.8747, 0.1741, 0.7187, 0.9810]]),\n",
       "  tensor(25)),\n",
       " (tensor([[0.9917, 0.1338, 0.6962, 0.9199, 0.2899, 0.8899, 1.0000]]),\n",
       "  tensor(13)))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset, testset = torch.utils.data.random_split(dataset, [39664,4407])\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "trainset[0], trainset[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [8, 26]                   --\n",
       "├─Sequential: 1-1                        [8, 16, 3]                --\n",
       "│    └─Conv1d: 2-1                       [8, 4, 5]                 16\n",
       "│    └─Dropout1d: 2-2                    [8, 4, 5]                 --\n",
       "│    └─ReLU: 2-3                         [8, 4, 5]                 --\n",
       "│    └─Conv1d: 2-4                       [8, 8, 4]                 72\n",
       "│    └─Dropout1d: 2-5                    [8, 8, 4]                 --\n",
       "│    └─ReLU: 2-6                         [8, 8, 4]                 --\n",
       "│    └─Conv1d: 2-7                       [8, 16, 3]                272\n",
       "│    └─Dropout1d: 2-8                    [8, 16, 3]                --\n",
       "│    └─ReLU: 2-9                         [8, 16, 3]                --\n",
       "├─Linear: 1-2                            [8, 64]                   3,136\n",
       "├─Dropout: 1-3                           [8, 64]                   --\n",
       "├─Linear: 1-4                            [8, 26]                   1,690\n",
       "==========================================================================================\n",
       "Total params: 5,186\n",
       "Trainable params: 5,186\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.05\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 0.03\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3),\n",
    "                                    nn.Dropout1d(0.25),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv1d(in_channels=4, out_channels=8, kernel_size=2),\n",
    "                                    nn.Dropout1d(0.25),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv1d(in_channels=8, out_channels=16, kernel_size=2),\n",
    "                                    nn.Dropout1d(0.25),\n",
    "                                    nn.ReLU(),\n",
    "                                    )\n",
    "        \n",
    "        self.linear = nn.Linear(16 * 3, 64)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.out = nn.Linear(64, 26)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = nn.ReLU()(self.dropout(self.linear(x)))\n",
    "        x = self.out(x)\n",
    "        #x = nn.LogSoftmax()(x)\n",
    "        return x\n",
    "    \n",
    "test = Model()\n",
    "torchinfo.summary(test, (8, 1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(train_loader, model, epochs, loss_function):\n",
    "    writer = SummaryWriter()\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-05)\n",
    "    \n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            model_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (data,target) in enumerate(train_loader):\n",
    "                data = data.cuda().type(torch.cuda.FloatTensor)\n",
    "                               \n",
    "                onehot_target = F.one_hot(target, num_classes=26).cuda().type(torch.cuda.FloatTensor)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                prediction = model(data)\n",
    "                \n",
    "                loss = loss_function(prediction, onehot_target)\n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                model_loss += loss/len(train_loader)\n",
    "                \n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(**{'train__loss': model_loss.item()})   \n",
    "            writer.add_scalar(\"Train loss\", model_loss.item(), epoch)\n",
    "            \n",
    "            \n",
    "    \n",
    "    torch.save(model, \"ASL_interpreter.pt\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:57<00:00, 17.94s/it, train__loss=0.785]\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "epochs = 50\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "train(train_loader, model, epochs, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4171623159001305\n",
      "0.8706603131381893\n"
     ]
    }
   ],
   "source": [
    "def test(test_loader, model_name):\n",
    "    \n",
    "    file_to_load = model_name + \".pt\"\n",
    "    test_model = Model()\n",
    "    test_model = torch.load(file_to_load).cuda()\n",
    "    \n",
    "    prediction_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    test_model.eval()\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    for batch_idx, (data,target) in enumerate(test_loader):\n",
    "        \n",
    "        data = data.cuda().type(torch.cuda.FloatTensor)\n",
    "        onehot_target = F.one_hot(target, num_classes=26).cuda().type(torch.cuda.FloatTensor)\n",
    "        prediction = test_model(data).cuda()\n",
    "        \n",
    "        \n",
    "        loss += prediction_loss(prediction,onehot_target)\n",
    "        pred_class = prediction.data.max(1, keepdim=True)[1]\n",
    "        correct += pred_class.eq(target.cuda().data.view_as(pred_class)).sum()\n",
    "\n",
    "    print(loss.item()/len(testset))\n",
    "    print(correct.item()/len(testset))\n",
    "    \n",
    "\n",
    "test(test_loader, \"ASL_interpreter\")\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e145918ee62325bb783ae69933f229276b5458d89ab4d949d12da7376064d41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
